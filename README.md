# OllamaChat

A chat interface with local LLMs using Ollama.

![OllamaChat Screenshot](OLLAMA.png)

## Table of Contents
- [Introduction](#introduction)
- [Features](#features)
- [Installation](#installation)
  - [Prerequisites](#prerequisites)
  - [Setup](#setup)
- [Usage](#usage)
- [Contributing](#contributing)
- [License](#license)

## Introduction
OllamaChat is a chat interface designed to work with local Language Model Machines (LLMs) using Ollama. This project aims to provide a robust chat application with support for local LLMs, ensuring privacy and performance.

## Features
- Local LLM support using Ollama
- Easy setup and deployment with Django
- Secure and private communication

## Installation

### Prerequisites
Before you begin, ensure you have met the following requirements:
- Python 3.x
- Django
- Ollama

### Setup

1. **Clone the Repository**
   ```sh
   git clone [https://github.com/yourusername/OllamaChat.git](https://github.com/traromal/OllamaChat.git)
   cd OllamaChat
2. **Set Up a Virtual Environment**
   ```sh
   python3 -m venv venv
   source venv/bin/activate
3. ** Install Django and Other Dependencies**
   ```sh
   pip install django
4. **Configure Django**
   Navigate to the OllamaChat directory and create a new Django project:
   **Apply initial migrations:**
   ```sh
    python manage.py migrate
 5.**Run the Development Server**
  ```sh
     python manage.py runserver


  

